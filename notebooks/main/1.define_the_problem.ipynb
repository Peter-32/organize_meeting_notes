{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task**: Predict whether a credit card transaction is fraud.\n",
    "- **Performance**: Maximize recall given 5% precision\n",
    "- **Experience**: Credit card transactions\n",
    "- **Similar Projects**: Detecting churn\n",
    "- **Assumptions**: Amount and the first principle components will be the most important variables.  Shap feature importance will be a good start.  I don't think two days of time data is enough to get a generalizable feature from time given that I believe time is mainly a good fraud feature for checking day of the week and hour of the day.\n",
    "- **Why**: Banks are liable for credit card fraud over $50, so they must minimize this cost by catching it with machines.\n",
    "- **Benefits**: Being able to stop a transaction suspected of fraud.  We can freeze a card and investigate it for fraud.  The model will improve as we find more fraud using the model.\n",
    "- **Handoff**: We will handoff this model by saving it to a file and uploading the model to our server.  We will run a prediction after every transaction, so we can reject fradulent transactions, freeze the account, and update the database which alerts the investigations department.\n",
    "- **Buy-in**: We need buy-in from the engineering department responsible for code that runs after every transaction.  We also need buy-in from investigators, to make sure they will be willing to investigate and resolve these frozen accounts.\n",
    "- **Data Available**: time in seconds (over the course of 2 days), amount, 28 principle components from PCA\n",
    "- **Data I Wish I Had**: whether the chip reader was used, whether the chip reader was attempted to be used, item purchased, time of day purchased, user history data\n",
    "- **Data to Ignore**: drop time in seconds.  Drop a lot of the principle components, but first check for Shap importance of all of them\n",
    "- **Development Data Format**: CSV\n",
    "- **Missing Data**: None\n",
    "- **Anonymize Data**: Already done\n",
    "- **Change Datatypes**: Not necessary, small file of numbers\n",
    "- **Test Set Size**: Dev set and test set will be 10,0000 samples each\n",
    "- **Training Set Sampling Plan**: No sampling.  Training will be approximately 300,000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244934, 30) (19936, 30) (19937, 30)\n",
      "\n",
      "Train\n",
      " 0    244511\n",
      "1       423\n",
      "Name: target, dtype: int64 \n",
      "\n",
      "Dev\n",
      " 0    19902\n",
      "1       34\n",
      "Name: target, dtype: int64 \n",
      "\n",
      "Test\n",
      " 0    19902\n",
      "1       35\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed - ensures that the datasets are split the same way if re-run\n",
    "seed(32)\n",
    "\n",
    "# Read in data\n",
    "df = pd.read_csv(\"../../data/raw/creditcard.csv\")\n",
    "\n",
    "# Common function to improve column names\n",
    "def camel_to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "df.columns = [camel_to_snake_case(x) for x in df.columns]\n",
    "df.rename(columns={'class': 'target'}, inplace=True)\n",
    "\n",
    "# Drop columns\n",
    "df.drop([\"time\"], axis=1, inplace=True)\n",
    "\n",
    "# Split datasets\n",
    "train, temp = train_test_split(df, test_size=0.14, stratify=df['target'])\n",
    "dev, test = train_test_split(temp, test_size=0.50, stratify=temp['target'])\n",
    "\n",
    "# Write results to files\n",
    "train.to_csv(\"../../data/interim/train.csv\", index=False)\n",
    "dev.to_csv(\"../../data/interim/dev.csv\", index=False)\n",
    "test.to_csv(\"../../data/interim/test.csv\", index=False)\n",
    "\n",
    "# Print the dataframe shapes and show the rows per target value\n",
    "print(train.shape, dev.shape, test.shape)\n",
    "print(\"\\nTrain\\n\", train['target'].value_counts(), \"\\n\\nDev\\n\", dev['target'].value_counts(), \"\\n\\nTest\\n\", test['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commons",
   "language": "python",
   "name": "commons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
